{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open-domain question answering with DeepPavlov\n",
    "\n",
    "\n",
    "This notebook shows how to train and interact with the Open Domain Question Answering (ODQA) component of the DeepPavlov framework. The DeepPavlov ODQA system has a modular architecture where the ranker is based on the **DrQA** [1] approach proposed by Facebook Research and the reader is based on **R-NET** [2] proposed by Microsoft Research Asia and its implementation by Wenxuan Zhou. The DeepPavlov framework contains pretrained models to extract answers from Wikipedia for Russian and English.\n",
    "\n",
    "[1]\n",
    "[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Requirements\n",
    "\n",
    "Both Wikipedia-based models require about 24 GB of RAM. It is possible to run them on a 16 GB machine, but the swap size should be at least 8 GB. You can run both models on Google Colab enabling the Hardware accelerator in Edit->Notebook settings.\n",
    "\n",
    " \n",
    "But first, install DeepPavlov and all the model's requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install -q deeppavlov\n",
    "!python -m deeppavlov install en_odqa_infer_wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Description\n",
    "\n",
    "The architecture of the ODQA skill is modular and consists of two components, a **ranker** and a **reader**. In order to answer any question, the **reader** first retrieves **top_n** relevant articles from the document collection, and then the **reader** scans them carefully to identify the answer. The detailed description of the ODQA models can be found in the [DeepPavlov documentation](http://docs.deeppavlov.ai/en/master/skills/odqa.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load https://raw.githubusercontent.com/deepmipt/DeepPavlov/master/deeppavlov/configs/odqa/en_odqa_infer_wiki.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with the model\n",
    "\n",
    "The DeepPavlov ODQA system has two Wikipedia-based models. The English Wikipedia model (enwiki.db) requires ~9.5 GB of local storage, whereas the Russian version takes ~2.7 GB of local storage. The Wikipedia dumps can be rebuilt by steps described in the [documentation](http://docs.deeppavlov.ai/en/master/components/tfidf_ranking.html#available-data-and-pretrained-models).\n",
    "\n",
    "Make sure that you can navigate the configuration files by using Autocomplete (Tab key) with **configs** module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov import configs\n",
    "from deeppavlov.core.commands.infer import build_model\n",
    "\n",
    "odqa = build_model(configs.odqa.en_odqa_infer_wiki, download = True)\n",
    "a = odqa([\"Who destroyed the Death Star?\"])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "\n",
    "You can train a model by running the framework with **train** parameter, wherein the model will be trained on the document collection defined in the **dataset_reader** section of the configuration file. DeepPavlov ODQA supports three types of document sources: \n",
    "\n",
    "* *wiki* - The Wikipedia dump\n",
    "* *txt* - each document in separate txt file\n",
    "* *json* - JSON files should be formatted as list with dicts which contain 'title' and 'doc' keywords.\n",
    "\n",
    "Let's train the model on the PloS sentence corpus [3]. This corpus consists of 300 computatioal biolog articles where each article stored in a separate *txt* file.\n",
    "\n",
    "[3]: A. Chambers. Statistical models for text classification: Applications and analysis, Ph.D., University of California, Irvine (2013). ProQuest Dissertations and Theses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget -q http://archive.ics.uci.edu/ml/machine-learning-databases/00311/SentenceCorpus.zip\n",
    "!unzip SentenceCorpus.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we will use the same configuration files that are used for the Wikipedia-based ODQA system; however, we strongly encourage you to create custom configuration files for your own models. In order to fit a model on new data, first, change the **data_path** parameter of the **dataset_reader** section. Then change the **dataset_format** to *txt*. In addition, you can alter the number of **top_n** documents to retrieve. Finally, train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deeppavlov import configs\n",
    "from deeppavlov.core.common.file import read_json\n",
    "from deeppavlov import configs, train_model\n",
    "\n",
    "model_config = read_json(configs.doc_retrieval.en_ranker_tfidf_wiki)\n",
    "model_config[\"dataset_reader\"][\"data_path\"] = \"/content/SentenceCorpus/unlabeled_articles/plos_unlabeled\"\n",
    "model_config[\"dataset_reader\"][\"dataset_format\"] = \"txt\"\n",
    "model_config[\"chainer\"][-1][\"top_n\"] = 30\n",
    "doc_retrieval = train_model(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the ranker output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_retrieval(['cerebellum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is done to run the ODQA component, make sure that the **download=Flase** otherwise the pretrained Wikipedia dump will overwrite your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deeppavlov import configs\n",
    "from deeppavlov.core.commands.infer import build_model\n",
    "\n",
    "odqa = build_model(configs.odqa.en_odqa_infer_wiki, download = False)\n",
    "a = odqa([\"what is tuberculosis ?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Us\n",
    "\n",
    "We are iPavlov, our story started in 2017 when we decided to build a conversational AI framework that on the one hand will contain all required NLP components to build chatbots and on the other hand will be easy to use. Our work resulted in releasing DeepPavlov library. Our lab at MIPT is honored with Facebook AI Academic Partnership and NVIDIA GPU Research Center status. We successfully combine research and extreme coding in our week-long DeepHack.me hackathons — DeepHack.Game, DeepHack.Q&A and DeepHack.RL. We serve a global AI community by organizing NIPS Conversational Challenge to evaluate state-of-the-art techniques in the field of dialog systems and collect open source dialog datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful links\n",
    "\n",
    "[DeepPavlov repository](https://github.com/deepmipt/DeepPavlov)\n",
    "\n",
    "[DeepPavlov demo page](https://demo.ipavlov.ai)\n",
    "\n",
    "[DeepPavlov documentation](https://docs.deeppavlov.ai)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
